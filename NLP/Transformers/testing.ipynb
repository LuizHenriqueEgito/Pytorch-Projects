{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a0f91a-d97a-48c3-907a-7cf9a12da968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Configuração da camada de atenção\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "\n",
    "        # Camada de avanço\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, embed_dim),\n",
    "        )\n",
    "\n",
    "        # Normalização\n",
    "        self.norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm_2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ativação de auto-atendimento (self-attention)\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm_1(x)\n",
    "\n",
    "        # Feedforward\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.norm_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Exemplo de uso:\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "hidden_size = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "encoder_block = EncoderBlock(embed_dim, num_heads, hidden_size, dropout)\n",
    "\n",
    "# Testando com um tensor de entrada\n",
    "input_tensor = torch.randn((10, 32, embed_dim))  # (sequence_length, batch_size, embed_dim)\n",
    "output_tensor = encoder_block(input_tensor)\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f671e74-f56e-4903-81a2-7b2342c123c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.0, max_len: int = 1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class FeedFowardBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size, dropout: int = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff_1 = nn.Linear(embed_dim, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ff_2 = nn.Linear(hidden_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ff_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.ff_2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_size, dropout: int = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout\n",
    "            )\n",
    "\n",
    "        self.feedforward = FeedFowardBlock(\n",
    "                embed_dim=embed_dim, hidden_size=hidden_size, dropout=dropout\n",
    "            )\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "        self.norm_2 = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_1(x + self.attention(x))\n",
    "        x = self.norm_2(x + self.feedforward(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_len,\n",
    "        embed_dim,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        hidden_size,\n",
    "        dropout: int = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            d_model=embed_dim,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "        )\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.encoder_blocks.append(\n",
    "                EncoderBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    hidden_size=hidden_size,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70e20157-5c9e-4b35-8341-80f57a3edae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: treinamos um encoder para aprender a linguagem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "VOCAB_SIZE = 30_000\n",
    "MAX_LEN = 16\n",
    "D_MODEL = 32\n",
    "N_LAYERS = 8\n",
    "N_HEADS = 8\n",
    "HIDDEN_SIZE = 4 \n",
    "\n",
    "\n",
    "class PreTrainedDairai(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int = VOCAB_SIZE,\n",
    "            max_len: int = MAX_LEN,\n",
    "            embed_dim: int = D_MODEL,\n",
    "            num_layers: int = N_LAYERS,\n",
    "            num_heads: int = N_HEADS,\n",
    "            hidden_size: int = HIDDEN_SIZE,\n",
    "            dropout: float = 0.05\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            max_len=max_len,\n",
    "            embed_dim=embed_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.mlm_head = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        masked_ids = torch.flatten(mask.reshape((-1,)).nonzero())\n",
    "        last_hidden_states = self.encoder(x)\n",
    "        all_hidden_states = last_hidden_states.reshape(-1, self.embed_dim)\n",
    "        masked_hidden_states = all_hidden_states[masked_ids, :]\n",
    "        logits = self.mlm_head(masked_hidden_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30e725f6-11f8-4773-b0af-d80ad810e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PreTrainedDairai(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_len=MAX_LEN,\n",
    "        embed_dim=D_MODEL,\n",
    "        num_layers=N_LAYERS,\n",
    "        num_heads=N_HEADS,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4608c24-8af3-462d-af16-d555f28957f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedDairai(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(30000, 32)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-7): 8 x EncoderBlock(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedFowardBlock(\n",
       "          (ff_1): Linear(in_features=32, out_features=4, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ff_2): Linear(in_features=4, out_features=32, bias=True)\n",
       "        )\n",
       "        (norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlm_head): Linear(in_features=32, out_features=30000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "346325c7-3a64-4d20-a67f-504cf0c3d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn((MAX_LEN, 4, D_MODEL))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5bfb24c-18ab-45ac-bc54-bd09d476aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([False, False,  True, False, False,  True, False, False,  True, False,\n",
    "        False, False,  True, False,  True, False, False, False, False, False,\n",
    "        False, False, False, False, False, False, False, False, False, False,\n",
    "        False,  True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa9624a-c845-4646-a61e-623f8ad10bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412b24b-0fe6-4d49-b93d-47f8c739c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MultiheadAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ef53d0-907a-419f-90c5-555c2e7f2799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7828, -0.9739,  1.2806,  ..., -0.3408, -0.3639,  0.5790],\n",
       "         [ 0.7138, -0.7522, -1.2334,  ..., -0.7771,  1.6783,  0.6866],\n",
       "         [-1.1697, -0.6700, -0.9719,  ...,  1.2864, -0.2909,  2.4255],\n",
       "         [ 0.2447, -0.7539,  2.0695,  ...,  1.6358, -0.5475,  2.3103]],\n",
       "\n",
       "        [[-1.0699,  0.2901,  0.0422,  ...,  0.6257,  0.7616,  1.2917],\n",
       "         [-0.4748, -0.6093,  1.7276,  ..., -0.6071,  1.2183, -1.6340],\n",
       "         [ 1.4023, -0.3572,  0.4537,  ..., -0.3655, -1.3445, -1.0162],\n",
       "         [ 0.7138, -0.8601,  1.9202,  ...,  0.1581, -0.4403, -0.7742]],\n",
       "\n",
       "        [[-0.3864,  0.1110,  0.1057,  ...,  0.2295, -0.4114, -0.0696],\n",
       "         [ 1.5512,  1.6452, -0.9075,  ...,  1.2767, -0.6229, -0.3755],\n",
       "         [-0.0273,  0.7276,  0.4164,  ...,  0.2826, -0.1423,  1.1013],\n",
       "         [-1.7906, -2.4617, -0.1253,  ...,  0.1300,  0.0429, -0.3763]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.7467,  1.5654, -0.0824,  ...,  0.0484,  1.3317, -0.3217],\n",
       "         [-0.3378, -0.2106, -0.9854,  ...,  0.7427,  1.0973, -0.3648],\n",
       "         [ 0.1758,  0.9340,  0.2361,  ...,  0.3902, -0.5298, -0.1867],\n",
       "         [ 0.0772,  1.8225, -0.0233,  ..., -1.1839,  0.6314, -0.4514]],\n",
       "\n",
       "        [[ 0.2819, -0.4954, -0.9992,  ...,  0.2796, -0.2066, -0.5480],\n",
       "         [ 1.1443,  1.2442, -0.0863,  ...,  0.1932, -0.8169,  1.1467],\n",
       "         [-0.3657,  1.6331,  1.6175,  ...,  0.8855, -0.0572, -0.1903],\n",
       "         [ 0.2106,  1.0730, -1.2274,  ...,  1.2801, -1.8942,  0.0866]],\n",
       "\n",
       "        [[-0.4216,  0.5295,  0.1150,  ...,  0.6713, -0.0597, -0.0148],\n",
       "         [-1.7520,  1.2599, -0.1858,  ...,  0.3654,  0.2318, -0.7054],\n",
       "         [ 2.8440,  0.4063, -0.4190,  ...,  0.5355, -0.4260, -0.3281],\n",
       "         [ 0.2392, -0.5718,  0.8466,  ..., -0.7349, -0.1110, -0.3035]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31b220f2-0f19-4bed-bacb-52c1099ad0cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36mPreTrainedDairai.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m     41\u001b[0m     masked_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(mask\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mnonzero())\n\u001b[1;32m---> 42\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m last_hidden_states\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m     44\u001b[0m     masked_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states[masked_ids, :]\n",
      "File \u001b[1;32mm:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 94\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 94\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(x)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_blocks:\n",
      "File \u001b[1;32mm:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mm:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "model(tensor, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e43071ee-7ed3-40a8-b0d2-c00c6c15b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72b4a683-dcee-4c96-9914-73aeac702a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "lxml not found, please install it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.fundamentus.com.br/resultado.php\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\pandas\\io\\html.py:1212\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend)\u001b[0m\n\u001b[0;32m   1208\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m   1210\u001b[0m io \u001b[38;5;241m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\pandas\\io\\html.py:977\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m retained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flav \u001b[38;5;129;01min\u001b[39;00m flavor:\n\u001b[1;32m--> 977\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43m_parser_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m     p \u001b[38;5;241m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mm:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\pandas\\io\\html.py:934\u001b[0m, in \u001b[0;36m_parser_dispatch\u001b[1;34m(flavor)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_LXML:\n\u001b[1;32m--> 934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml not found, please install it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _valid_parsers[flavor]\n",
      "\u001b[1;31mImportError\u001b[0m: lxml not found, please install it"
     ]
    }
   ],
   "source": [
    "pd.read_html('https://www.fundamentus.com.br/resultado.php')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93587a-839b-467c-9aef-6355952725cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
