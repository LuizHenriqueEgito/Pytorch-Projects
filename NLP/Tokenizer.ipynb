{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26ac5f50-9d5e-4d99-9fba-d02474e08a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.tokenizer import MyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae1c1aab-cd3e-4aa3-9775-cece1b8d70b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_text_iterator(data_path: str):\n",
    "    for arquivo_txt in Path(data_path).glob('**/*.txt'):\n",
    "        yield arquivo_txt.read_text(encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "875a913b-e3e6-461f-8d59-3b6691cdde3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer(tokenizer_path='src/tokenizer', vocab_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c213eddf-5e57-4f0e-be7f-a3d5fa70ef6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Let', (0, 3)), (\"'s\", (3, 5)), ('Ġtest', (5, 10)), ('Ġpre', (10, 14)), ('-', (14, 15)), ('tokenization', (15, 27)), ('!', (27, 28))]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(text_iterator=my_text_iterator(\"data_tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05625a5a-b1ff-489f-a31f-119e7f288a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texto original: I couldn't even create my own business, how awesome!\n",
      "texto tokenizado: [39, 14, 24, 30, 21, 15, 23, 29, 39, 16, 31, 16, 23, 39, 14, 27, 16, 13, 29, 16, 39, 22, 39, 24, 23, 39, 30, 28, 19, 23, 16, 28, 28, 39, 24, 39, 13, 16, 28, 24, 22, 16, 8]\n",
      "texto destokenizado:  couldnt even create m on usiness o aesome!\n"
     ]
    }
   ],
   "source": [
    "texto_exemplo = \"I couldn't even create my own business, how awesome!\"\n",
    "texto_tokenizado = tokenizer.tokenize_text(texto_exemplo)\n",
    "texto_destokenizado = tokenizer.untokenize_tokens(texto_tokenizado)\n",
    "print(f'texto original: {texto_exemplo}\\ntexto tokenizado: {texto_tokenizado}\\ntexto destokenizado: {texto_destokenizado}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7584b025-e973-4364-b17d-40339b9faf85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texto original: ola mundo!\n",
      "texto tokenizado: [24, 21, 13, 39, 22, 30, 23, 15, 24, 8]\n",
      "texto destokenizado: ola mundo!\n"
     ]
    }
   ],
   "source": [
    "texto_exemplo = \"ola mundo!\"\n",
    "texto_tokenizado = tokenizer.tokenize_text(texto_exemplo)\n",
    "texto_destokenizado = tokenizer.untokenize_tokens(texto_tokenizado)\n",
    "print(f'texto original: {texto_exemplo}\\ntexto tokenizado: {texto_tokenizado}\\ntexto destokenizado: {texto_destokenizado}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b08ed5fa-f1f7-4b49-b54f-f98d1c181cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'Olá eu sou o Luiz Egito!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a995e2e3-cc20-4985-bcdf-d02cca3ca78f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 21,\n",
       " 37,\n",
       " 34,\n",
       " 39,\n",
       " 16,\n",
       " 30,\n",
       " 39,\n",
       " 28,\n",
       " 24,\n",
       " 30,\n",
       " 39,\n",
       " 24,\n",
       " 39,\n",
       " 11,\n",
       " 30,\n",
       " 19,\n",
       " 33,\n",
       " 39,\n",
       " 10,\n",
       " 18,\n",
       " 19,\n",
       " 29,\n",
       " 24,\n",
       " 8]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_tokenizado = tokenizer.tokenize_text(texto)\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64a23bbf-d5c5-44fc-a4b4-686b8e7fe05b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá eu sou o Luiz Egito!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.untokenize_tokens(texto_tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c625e6d2-0cc4-432a-86bd-3098ce100b11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.untokenize_tokens([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d139027b-2d25-4bab-b019-710d1974ee90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 -> e\n",
      "21 -> l\n",
      "21 -> l\n",
      "24 -> o\n",
      "39 ->  \n",
      "24 -> o\n",
      "27 -> r\n",
      "21 -> l\n",
      "15 -> d\n",
      "39 ->  \n",
      "24 -> o\n",
      "22 -> m\n",
      "24 -> o\n",
      "39 ->  \n",
      "14 -> c\n",
      "24 -> o\n",
      "21 -> l\n",
      "24 -> o\n",
      "13 -> a\n",
      "27 -> r\n",
      "39 ->  \n",
      "39 ->  \n",
      "16 -> e\n",
      "39 ->  \n",
      "10 -> E\n"
     ]
    }
   ],
   "source": [
    "texto_exemplo = \"hello world. Como coloar UNK e SEP?\"\n",
    "texto_tokenizado = tokenizer.tokenize_text(texto_exemplo)\n",
    "for i in texto_tokenizado:\n",
    "    print(f'{i} -> {tokenizer.untokenize_tokens(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "114cf88c-97a3-4686-8725-611d99f7a744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texto original: hello world. Como coloar UNK e SEP?\n",
      "texto tokenizado: [16, 21, 21, 24, 39, 24, 27, 21, 15, 39, 24, 22, 24, 39, 14, 24, 21, 24, 13, 27, 39, 39, 16, 39, 10]\n",
      "texto destokenizado: ello orld omo coloar  e E\n"
     ]
    }
   ],
   "source": [
    "texto_destokenizado = tokenizer.untokenize_tokens(texto_tokenizado)\n",
    "print(f'texto original: {texto_exemplo}\\ntexto tokenizado: {texto_tokenizado}\\ntexto destokenizado: {texto_destokenizado}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cccc7c-869c-41f7-9c69-5ab3967f86fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
