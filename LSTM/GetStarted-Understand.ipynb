{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2192ce0e-caf1-4e4e-a465-124ef7bd36ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ab1515-2030-40d6-81d6-98b89f54ae80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.array([100, 110, 120, 125, 130, 140, 150, 155, 160, 170, 180, 190])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc44998-e45c-438e-a7c9-3964dab5fea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        targets.append(data[i + seq_length])\n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c6f386-e2d1-48e2-be72-817abe918ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_length = 5\n",
    "sequences, targets = create_sequences(data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa74ab0-92de-4b10-a214-bdcac715ba1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert sequences and targets to PyTorch tensors\n",
    "sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "targets = torch.tensor(targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d87384-bcef-48d6-bde2-e7224f9488b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_size = int(0.8 * len(sequences))\n",
    "train_sequences, test_sequences = sequences[:train_size], sequences[train_size:]\n",
    "train_targets, test_targets = targets[:train_size], targets[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcbcec0-72ae-4527-9df4-418420b27122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Get the last output in the sequence\n",
    "        return out\n",
    "\n",
    "input_size = 1  # Sales data is univariate\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = LSTMPredictor(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de8547ed-c737-4460-b188-c2ea96727bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f6e1cbd-d235-4dbe-b25f-a233ec5421a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 8543.8770\n",
      "Epoch [20/1000], Loss: 7596.9512\n",
      "Epoch [30/1000], Loss: 6709.9351\n",
      "Epoch [40/1000], Loss: 5869.7002\n",
      "Epoch [50/1000], Loss: 5116.1304\n",
      "Epoch [60/1000], Loss: 4438.6484\n",
      "Epoch [70/1000], Loss: 3835.0579\n",
      "Epoch [80/1000], Loss: 3300.3003\n",
      "Epoch [90/1000], Loss: 2828.8174\n",
      "Epoch [100/1000], Loss: 2415.1301\n",
      "Epoch [110/1000], Loss: 2053.9814\n",
      "Epoch [120/1000], Loss: 1740.3649\n",
      "Epoch [130/1000], Loss: 1469.5275\n",
      "Epoch [140/1000], Loss: 1236.9838\n",
      "Epoch [150/1000], Loss: 1038.5057\n",
      "Epoch [160/1000], Loss: 870.1417\n",
      "Epoch [170/1000], Loss: 728.2207\n",
      "Epoch [180/1000], Loss: 609.3572\n",
      "Epoch [190/1000], Loss: 510.4558\n",
      "Epoch [200/1000], Loss: 428.7119\n",
      "Epoch [210/1000], Loss: 361.6016\n",
      "Epoch [220/1000], Loss: 306.8807\n",
      "Epoch [230/1000], Loss: 262.5679\n",
      "Epoch [240/1000], Loss: 226.9305\n",
      "Epoch [250/1000], Loss: 198.4697\n",
      "Epoch [260/1000], Loss: 175.8987\n",
      "Epoch [270/1000], Loss: 158.1223\n",
      "Epoch [280/1000], Loss: 144.2212\n",
      "Epoch [290/1000], Loss: 133.4260\n",
      "Epoch [300/1000], Loss: 125.1010\n",
      "Epoch [310/1000], Loss: 118.7262\n",
      "Epoch [320/1000], Loss: 113.8789\n",
      "Epoch [330/1000], Loss: 110.2188\n",
      "Epoch [340/1000], Loss: 107.4744\n",
      "Epoch [350/1000], Loss: 105.4310\n",
      "Epoch [360/1000], Loss: 103.9202\n",
      "Epoch [370/1000], Loss: 102.8108\n",
      "Epoch [380/1000], Loss: 102.0019\n",
      "Epoch [390/1000], Loss: 101.4163\n",
      "Epoch [400/1000], Loss: 100.9952\n",
      "Epoch [410/1000], Loss: 100.6946\n",
      "Epoch [420/1000], Loss: 100.4815\n",
      "Epoch [430/1000], Loss: 100.3315\n",
      "Epoch [440/1000], Loss: 100.2267\n",
      "Epoch [450/1000], Loss: 100.1540\n",
      "Epoch [460/1000], Loss: 100.1039\n",
      "Epoch [470/1000], Loss: 100.0695\n",
      "Epoch [480/1000], Loss: 100.0463\n",
      "Epoch [490/1000], Loss: 100.0305\n",
      "Epoch [500/1000], Loss: 100.0200\n",
      "Epoch [510/1000], Loss: 100.0130\n",
      "Epoch [520/1000], Loss: 100.0084\n",
      "Epoch [530/1000], Loss: 100.0054\n",
      "Epoch [540/1000], Loss: 100.0034\n",
      "Epoch [550/1000], Loss: 100.0022\n",
      "Epoch [560/1000], Loss: 100.0014\n",
      "Epoch [570/1000], Loss: 100.0009\n",
      "Epoch [580/1000], Loss: 100.0005\n",
      "Epoch [590/1000], Loss: 100.0003\n",
      "Epoch [600/1000], Loss: 100.0002\n",
      "Epoch [610/1000], Loss: 100.0001\n",
      "Epoch [620/1000], Loss: 100.0001\n",
      "Epoch [630/1000], Loss: 100.0000\n",
      "Epoch [640/1000], Loss: 100.0000\n",
      "Epoch [650/1000], Loss: 100.0000\n",
      "Epoch [660/1000], Loss: 100.0000\n",
      "Epoch [670/1000], Loss: 100.0000\n",
      "Epoch [680/1000], Loss: 100.0000\n",
      "Epoch [690/1000], Loss: 100.0000\n",
      "Epoch [700/1000], Loss: 100.0000\n",
      "Epoch [710/1000], Loss: 100.0000\n",
      "Epoch [720/1000], Loss: 100.0000\n",
      "Epoch [730/1000], Loss: 100.0000\n",
      "Epoch [740/1000], Loss: 100.0000\n",
      "Epoch [750/1000], Loss: 100.0000\n",
      "Epoch [760/1000], Loss: 100.0000\n",
      "Epoch [770/1000], Loss: 100.0000\n",
      "Epoch [780/1000], Loss: 100.0000\n",
      "Epoch [790/1000], Loss: 100.0000\n",
      "Epoch [800/1000], Loss: 100.0000\n",
      "Epoch [810/1000], Loss: 100.0000\n",
      "Epoch [820/1000], Loss: 100.0000\n",
      "Epoch [830/1000], Loss: 100.0000\n",
      "Epoch [840/1000], Loss: 100.0000\n",
      "Epoch [850/1000], Loss: 100.0000\n",
      "Epoch [860/1000], Loss: 100.0000\n",
      "Epoch [870/1000], Loss: 100.0000\n",
      "Epoch [880/1000], Loss: 100.0000\n",
      "Epoch [890/1000], Loss: 100.0000\n",
      "Epoch [900/1000], Loss: 100.0000\n",
      "Epoch [910/1000], Loss: 100.0000\n",
      "Epoch [920/1000], Loss: 100.0000\n",
      "Epoch [930/1000], Loss: 100.0000\n",
      "Epoch [940/1000], Loss: 100.0000\n",
      "Epoch [950/1000], Loss: 100.0000\n",
      "Epoch [960/1000], Loss: 100.0000\n",
      "Epoch [970/1000], Loss: 100.0000\n",
      "Epoch [980/1000], Loss: 100.0000\n",
      "Epoch [990/1000], Loss: 100.0000\n",
      "Epoch [1000/1000], Loss: 100.0000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(train_sequences.unsqueeze(2))\n",
    "    loss = criterion(outputs, train_targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5176c0f4-075e-40a2-8917-a6b04cc4c92c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 925.0060\n",
      "Predicted Sales: [[154.99991]\n",
      " [154.9999 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_sequences.unsqueeze(2))\n",
    "    mse = criterion(test_outputs, test_targets)\n",
    "    print(f'Mean Squared Error on Test Data: {mse:.4f}')\n",
    "\n",
    "    # Convert the test_outputs to a NumPy array\n",
    "    predictions = test_outputs.numpy()\n",
    "    print('Predicted Sales:', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994c2ee7-ef0f-4ba7-9b40-e9d2731455b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([180., 190.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c41270f8-2757-4f58-833a-eeda6d84b72e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 925.0062\n",
      "Predicted Sales: [[154.9999 ]\n",
      " [154.9999 ]\n",
      " [154.9999 ]\n",
      " [154.9999 ]\n",
      " [154.99991]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\disco m\\python\\pythonprojects\\pytorch\\env_pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(train_sequences.unsqueeze(2))\n",
    "    mse = criterion(test_outputs, test_targets)\n",
    "    print(f'Mean Squared Error on Test Data: {mse:.4f}')\n",
    "\n",
    "    # Convert the test_outputs to a NumPy array\n",
    "    predictions = test_outputs.numpy()\n",
    "    print('Predicted Sales:', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77174e6-d367-4e9b-a939-b4052504d2f1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9a6597c-b52f-429f-b845-b107e9a2ac6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Price': [100, 105, 110, 120, 115, 112, 108, 109, 105, 102],\n",
    "    'Label': [0, 0, 1, 1, 1, 0, 0, 1, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c78b1196-ec6a-47e2-a2ae-2cca16500b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PriceLabelDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=3):\n",
    "        self.data = data.values  # Convert to numpy array\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prices = self.data[idx:idx + self.sequence_length, 0]\n",
    "        label = self.data[idx + self.sequence_length, 1]\n",
    "        return torch.FloatTensor(prices), torch.LongTensor([label])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = PriceLabelDataset(df, sequence_length=3)\n",
    "\n",
    "# Create a data loader\n",
    "batch_size = 2  # Adjust as needed\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96c6885a-2dde-4ab3-b2dd-1af90514508d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x.view(x.size(0), -1, 1), (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Create the model\n",
    "input_size = 1\n",
    "hidden_size = 8\n",
    "num_layers = 2\n",
    "output_size = 2\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab7f7622-dde3-450e-ae25-480ea725a5bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7260\n",
      "Epoch [2/100], Loss: 0.7233\n",
      "Epoch [3/100], Loss: 0.7218\n",
      "Epoch [4/100], Loss: 0.7206\n",
      "Epoch [5/100], Loss: 0.7195\n",
      "Epoch [6/100], Loss: 0.7184\n",
      "Epoch [7/100], Loss: 0.7174\n",
      "Epoch [8/100], Loss: 0.7164\n",
      "Epoch [9/100], Loss: 0.7154\n",
      "Epoch [10/100], Loss: 0.7145\n",
      "Epoch [11/100], Loss: 0.7135\n",
      "Epoch [12/100], Loss: 0.7126\n",
      "Epoch [13/100], Loss: 0.7117\n",
      "Epoch [14/100], Loss: 0.7109\n",
      "Epoch [15/100], Loss: 0.7100\n",
      "Epoch [16/100], Loss: 0.7091\n",
      "Epoch [17/100], Loss: 0.7083\n",
      "Epoch [18/100], Loss: 0.7074\n",
      "Epoch [19/100], Loss: 0.7066\n",
      "Epoch [20/100], Loss: 0.7058\n",
      "Epoch [21/100], Loss: 0.7051\n",
      "Epoch [22/100], Loss: 0.7043\n",
      "Epoch [23/100], Loss: 0.7036\n",
      "Epoch [24/100], Loss: 0.7029\n",
      "Epoch [25/100], Loss: 0.7023\n",
      "Epoch [26/100], Loss: 0.7017\n",
      "Epoch [27/100], Loss: 0.7011\n",
      "Epoch [28/100], Loss: 0.7005\n",
      "Epoch [29/100], Loss: 0.7000\n",
      "Epoch [30/100], Loss: 0.6996\n",
      "Epoch [31/100], Loss: 0.6992\n",
      "Epoch [32/100], Loss: 0.6988\n",
      "Epoch [33/100], Loss: 0.6984\n",
      "Epoch [34/100], Loss: 0.6980\n",
      "Epoch [35/100], Loss: 0.6977\n",
      "Epoch [36/100], Loss: 0.6974\n",
      "Epoch [37/100], Loss: 0.6972\n",
      "Epoch [38/100], Loss: 0.6969\n",
      "Epoch [39/100], Loss: 0.6967\n",
      "Epoch [40/100], Loss: 0.6965\n",
      "Epoch [41/100], Loss: 0.6963\n",
      "Epoch [42/100], Loss: 0.6961\n",
      "Epoch [43/100], Loss: 0.6959\n",
      "Epoch [44/100], Loss: 0.6958\n",
      "Epoch [45/100], Loss: 0.6956\n",
      "Epoch [46/100], Loss: 0.6955\n",
      "Epoch [47/100], Loss: 0.6954\n",
      "Epoch [48/100], Loss: 0.6952\n",
      "Epoch [49/100], Loss: 0.6951\n",
      "Epoch [50/100], Loss: 0.6950\n",
      "Epoch [51/100], Loss: 0.6949\n",
      "Epoch [52/100], Loss: 0.6948\n",
      "Epoch [53/100], Loss: 0.6948\n",
      "Epoch [54/100], Loss: 0.6947\n",
      "Epoch [55/100], Loss: 0.6946\n",
      "Epoch [56/100], Loss: 0.6945\n",
      "Epoch [57/100], Loss: 0.6945\n",
      "Epoch [58/100], Loss: 0.6944\n",
      "Epoch [59/100], Loss: 0.6943\n",
      "Epoch [60/100], Loss: 0.6943\n",
      "Epoch [61/100], Loss: 0.6942\n",
      "Epoch [62/100], Loss: 0.6942\n",
      "Epoch [63/100], Loss: 0.6941\n",
      "Epoch [64/100], Loss: 0.6941\n",
      "Epoch [65/100], Loss: 0.6940\n",
      "Epoch [66/100], Loss: 0.6940\n",
      "Epoch [67/100], Loss: 0.6940\n",
      "Epoch [68/100], Loss: 0.6939\n",
      "Epoch [69/100], Loss: 0.6939\n",
      "Epoch [70/100], Loss: 0.6939\n",
      "Epoch [71/100], Loss: 0.6938\n",
      "Epoch [72/100], Loss: 0.6938\n",
      "Epoch [73/100], Loss: 0.6938\n",
      "Epoch [74/100], Loss: 0.6938\n",
      "Epoch [75/100], Loss: 0.6937\n",
      "Epoch [76/100], Loss: 0.6937\n",
      "Epoch [77/100], Loss: 0.6937\n",
      "Epoch [78/100], Loss: 0.6937\n",
      "Epoch [79/100], Loss: 0.6937\n",
      "Epoch [80/100], Loss: 0.6936\n",
      "Epoch [81/100], Loss: 0.6936\n",
      "Epoch [82/100], Loss: 0.6936\n",
      "Epoch [83/100], Loss: 0.6936\n",
      "Epoch [84/100], Loss: 0.6936\n",
      "Epoch [85/100], Loss: 0.6936\n",
      "Epoch [86/100], Loss: 0.6935\n",
      "Epoch [87/100], Loss: 0.6935\n",
      "Epoch [88/100], Loss: 0.6935\n",
      "Epoch [89/100], Loss: 0.6935\n",
      "Epoch [90/100], Loss: 0.6935\n",
      "Epoch [91/100], Loss: 0.6935\n",
      "Epoch [92/100], Loss: 0.6935\n",
      "Epoch [93/100], Loss: 0.6935\n",
      "Epoch [94/100], Loss: 0.6935\n",
      "Epoch [95/100], Loss: 0.6935\n",
      "Epoch [96/100], Loss: 0.6935\n",
      "Epoch [97/100], Loss: 0.6934\n",
      "Epoch [98/100], Loss: 0.6934\n",
      "Epoch [99/100], Loss: 0.6934\n",
      "Epoch [100/100], Loss: 0.6934\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for prices, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(prices)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468708da-cac2-4642-b2b2-2610c7de06a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
